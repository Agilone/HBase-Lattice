/*
 * 
 *  Copyright Â© 2010, 2011 Inadco, Inc. All rights reserved.
 *  
 *     Licensed under the Apache License, Version 2.0 (the "License");
 *     you may not use this file except in compliance with the License.
 *     You may obtain a copy of the License at
 *  
 *         http://www.apache.org/licenses/LICENSE-2.0
 *  
 *     Unless required by applicable law or agreed to in writing, software
 *     distributed under the License is distributed on an "AS IS" BASIS,
 *     WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 *     See the License for the specific language governing permissions and
 *     limitations under the License.
 *  
 *  
 */
package com.inadco.hbl.client.impl;

import java.io.IOException;
import java.util.ArrayList;
import java.util.HashMap;
import java.util.HashSet;
import java.util.List;
import java.util.Map;
import java.util.Set;
import java.util.concurrent.ExecutorService;

import org.apache.commons.lang.Validate;
import org.apache.hadoop.hbase.client.HTablePool;
import org.apache.hadoop.hbase.util.Bytes;

import com.inadco.hbl.api.AggregateFunctionRegistry;
import com.inadco.hbl.api.Cube;
import com.inadco.hbl.api.Cuboid;
import com.inadco.hbl.api.Dimension;
import com.inadco.hbl.api.Range;
import com.inadco.hbl.client.AggregateQuery;
import com.inadco.hbl.client.AggregateResultSet;
import com.inadco.hbl.client.HblException;
import com.inadco.hbl.client.HblQueryClient;
import com.inadco.hbl.client.impl.scanner.ScanSpec;

import coprocessor.results.CompositeRawScanResultComparator;
import coprocessor.results.RawScanResultMeasureComparator;

/**
 * Projection query implementation.
 * <P>
 * 
 * @author dmitriy
 * 
 */
public class AggregateQueryImpl implements AggregateQuery {

	protected HblQueryClient            client;
	protected Cube                      cube;
	private ExecutorService             es;
	/**
	 * dim name -> range slice requested
	 */
	private Map<String, List<Slice>>    dimSlices       = new HashMap<String, List<Slice>>();
	private Set<String>                 measures        = new HashSet<String>();

	protected List<String>              groupDimensions = new ArrayList<String>();
	private HTablePool                  tpool;
	protected AggregateFunctionRegistry afr;
	protected boolean                   allowComplements;
	protected int 						limit = Integer.MAX_VALUE;
	protected String					distinctDimension;
	protected int[][]					keyOffsets;

	protected CompositeRawScanResultComparator comparator = new CompositeRawScanResultComparator(); 

	public AggregateQueryImpl(HblQueryClient client, ExecutorService es, HTablePool tpool) {
		super();
		this.es = es;
		this.tpool = tpool;
		this.client = client;
	}

	public AggregateQuery setCube(String cubeName) throws HblException {
		if (cube == null || !cube.getName().equals(cubeName)) {
			cube = client.getCube(cubeName);
			afr = cube.getAggregateFunctionRegistry();
		}
		return this;
	}

	@Override
	public AggregateQuery addMeasure(String measure) {
		Validate.notNull(measure);
		Validate.notNull(cube, "A cube not set");
		Validate.isTrue(cube.getMeasures().containsKey(measure), "Unknown measure name");
		measures.add(measure);
		return this;
	}

	@Override
	public AggregateQuery addGroupBy(String dimName) {
		Validate.notNull(dimName);
		Validate.notNull(cube, "A cube not set");
		Validate.isTrue(cube.getDimensions().containsKey(dimName), "no such dimension found");

		groupDimensions.add(dimName);
		return this;
	}

	@Override
	public AggregateQuery addClosedSlice(String dimension, Object leftBound, Object rightBound) {
		return addSlice(dimension, leftBound, false, rightBound, false);
	}

	@Override
	public AggregateQuery addOpenSlice(String dimension, Object leftBound, Object rightBound) {
		return addSlice(dimension, leftBound, true, rightBound, true);
	}

	@Override
	public AggregateQuery addHalfOpenSlice(String dimension, Object leftBound, Object rightBound) {
		return addSlice(dimension, leftBound, false, rightBound, true);
	}

	@Override
	public AggregateQuery addSlice(String dimension,
			Object leftBound,
			boolean leftOpen,
			Object rightBound,
			boolean rightOpen) {
		Validate.notNull(dimension);
		Validate.notNull(cube, "A cube not set");
		Validate.isTrue(cube.getDimensions().containsKey(dimension));

		if (leftBound == null && rightBound == null) {
			dimSlices.remove(dimension);
			return this;
		}
		List<Slice> sliceSet = dimSlices.get(dimension);
		if (sliceSet == null)
			dimSlices.put(dimension, sliceSet = new ArrayList<Slice>(4));
		sliceSet.add(new Slice(leftBound, leftOpen, rightBound, rightOpen));

		return this;
	}

	public List<ScanSpec> generateScanSpecs(Map<String, Integer> dimName2GroupKeyOffsetMap,
			Map<String, Integer> measureName2indexMap) throws IOException, HblException {
		if (dimName2GroupKeyOffsetMap == null)
			dimName2GroupKeyOffsetMap = new HashMap<String, Integer>(11);
		if (measureName2indexMap == null)
			measureName2indexMap = new HashMap<String, Integer>(11);

		Validate.notNull(cube, "A cube not set");
		Cuboid cuboid = findCuboid();

		Validate.notNull(cuboid, "Unable to find a suitable cuboid for the slice query.");

		/*
		 * FIXME, TODO: check slices for overlapping. otherwise, if slices
		 * overlap, not only we'd be performing more scans than needed, but they
		 * will also contain duplicate counts.
		 * 
		 * for now we just have to assume that slices will not overlap.
		 */
		List<ScanSpec> scanSpecs = new ArrayList<ScanSpec>();

		List<Range> partialSpec = new ArrayList<Range>();

		int groupKeyLen = 0, curKeyLen = 0;


		if(this.distinctDimension != null) {
			keyOffsets = new int[2][groupDimensions.size()];
		}

		int dCnt = 0;
		for (Dimension dim : cuboid.getCuboidDimensions()) {
			String dimName = dim.getName();

			if(this.distinctDimension != null && !this.distinctDimension.equals(dim.getName())) {
				keyOffsets[0][dCnt] = curKeyLen;
				keyOffsets[1][dCnt] = dim.getKeyLen();
				dCnt++;
			} else if(this.distinctDimension != null && this.distinctDimension.equals(dim.getName())) {
				keyOffsets[0][groupDimensions.size()-1] = curKeyLen;
				keyOffsets[1][groupDimensions.size()-1] = dim.getKeyLen();
			}

			dimName2GroupKeyOffsetMap.put(dimName, curKeyLen);
			curKeyLen += dim.getKeyLen();

			if (groupDimensions.contains(dim.getName())) {
				groupKeyLen = curKeyLen;
			}
		}

		byte[][] measureQualifiers = new byte[measures.size()][];
		int mCnt = 0;
		for (String mName : measures) {
			measureName2indexMap.put(mName, mCnt);
			measureQualifiers[mCnt++] = Bytes.toBytes(mName);
		}

		generateScanSpecs(cuboid, scanSpecs, partialSpec, 0, groupKeyLen, SliceOperation.ADD, measureQualifiers);

		return scanSpecs;

	}

	@Override
	public AggregateResultSet execute() throws HblException {
		try {
			Map<String, Integer> dimName2GroupKeyOffsetMap = new HashMap<String, Integer>();
			Map<String, Integer> measureName2indexMap = new HashMap<String, Integer>();
			List<ScanSpec> scanSpecs = generateScanSpecs(dimName2GroupKeyOffsetMap, measureName2indexMap);

			return createResultSet(scanSpecs,
					es,
					tpool,
					afr,
					measureName2indexMap,
					dimName2GroupKeyOffsetMap,
					null,
					null,
					null);
		} catch (IOException exc) {
			throw new HblException(exc.getMessage(), exc);
		} finally {
			reset();
		}
	}

	public AggregateResultSet execute(byte[] startSplitKey, byte[] endSplitKey, String enforcedCuboidTableName)
			throws HblException {
		try {
			Map<String, Integer> dimName2GroupKeyOffsetMap = new HashMap<String, Integer>();
			Map<String, Integer> measureName2indexMap = new HashMap<String, Integer>();
			List<ScanSpec> scanSpecs = generateScanSpecs(dimName2GroupKeyOffsetMap, measureName2indexMap);

			return createResultSet(scanSpecs,
					es,
					tpool,
					afr,
					measureName2indexMap,
					dimName2GroupKeyOffsetMap,
					startSplitKey,
					endSplitKey,
					enforcedCuboidTableName);
		} catch (IOException exc) {
			throw new HblException(exc.getMessage(), exc);
		} finally {
			reset();
		}

	}

	protected void reset() {
		dimSlices.clear();
		measures.clear();
		groupDimensions.clear();
	}

	protected boolean isAllowComplements() {
		return allowComplements;
	}

	protected void setAllowComplements(boolean allowComplements) {
		this.allowComplements = allowComplements;
	}

	/**
	 * 
	 * @param scanSpecs
	 * @param es
	 * @param tpool
	 * @param afr
	 * @param measureName2IndexMap
	 * @param dimName2GroupKeyOffsetMap
	 * @param startSplitKey
	 *            optional: if given, enforce MR split constraints per half-open
	 *            [startSplitKey,endSplitKey). note that in this case
	 *            endSplitKey can still be null and means
	 *            "till the end of the table".
	 * @param endSplitKey
	 *            optional.
	 * @param enforcedCuboidTableName
	 *            optional. if passed on, ensures that the cuboid table selected
	 *            is the same as given, thus inforcing idempotent cuboid. Used
	 *            only by HblInputSplit to assert idempotent optimizer
	 *            processing.
	 * @return
	 * @throws IOException
	 */
	protected AggregateResultSetImpl createResultSet(final List<ScanSpec> scanSpecs,
			final ExecutorService es,
			final HTablePool tpool,
			final AggregateFunctionRegistry afr,
			final Map<String, Integer> measureName2IndexMap,
			final Map<String, Integer> dimName2GroupKeyOffsetMap,
			final byte[] startSplitKey,
			final byte[] endSplitKey,
			final String enforcedCuboidTableName) throws IOException {
		return new AggregateResultSetImpl(
				scanSpecs,
				es,
				tpool,
				afr,
				measureName2IndexMap,
				dimName2GroupKeyOffsetMap,
				startSplitKey,
				endSplitKey,
				enforcedCuboidTableName,
				keyOffsets,
				limit,
				comparator);
	}

	/**
	 * Generate cartesian product of all individual dimension scans and also
	 * flip scan operation between ADD and COMPLEMENT types
	 * 
	 * @param cuboid
	 * @param scanHolder
	 * @param partialSpec
	 * @param dimIndex
	 * @param numGroupKeys
	 * @param so
	 */
	private void generateScanSpecs(Cuboid cuboid,
			List<ScanSpec> scanHolder,
			List<Range> partialSpec,
			int dimIndex,
			int groupKeyLen,
			SliceOperation so,
			byte[][] measureQualifiers) {
		List<Dimension> dimensions = cuboid.getCuboidDimensions();
		if (dimIndex == dimensions.size()) {
			// add leaf
			scanHolder.add(new ScanSpec(
					measureQualifiers,
					groupKeyLen,
					partialSpec.toArray(new Range[dimIndex]),
					cuboid,
					so));
			return;
		}
		Dimension dim = dimensions.get(dimIndex);
		List<Slice> slices = dimSlices.get(dim.getName());
		if (slices == null) {

			// generate 'total' slice
			Range allRange = dim.allRange();

			if (partialSpec.size() == dimIndex)
				partialSpec.add(allRange);
			else
				partialSpec.set(dimIndex, allRange);

			generateScanSpecs(cuboid, scanHolder, partialSpec, dimIndex + 1, groupKeyLen, so, measureQualifiers);
		} else {
			if (slices.size() != 1)
				throw new UnsupportedOperationException(
						"queries to multiple slices of the same dimension are not supported (yet)!");
			Slice slice = slices.iterator().next();
			Range[] ranges = dim.optimizeSliceScan(slice, allowComplements);

			Validate.notEmpty(ranges);

			for (Range r : ranges) {
				SliceOperation nextSo = so;

				/*
				 * clarification:
				 * 
				 * we introduce slice operation (a complement operation in a
				 * plan or additive operation) just for the dimension to be able
				 * to advise us on most optimal combination of such. Note that a
				 * complement slice S as in A\S becomes a union as in A\(S\S2) =
				 * (A\S) U S2 if S2 \subset S. so due to similar argumentation
				 * accross different dimensions, we just invert slice operations
				 * in each subsequent dimension encountered (since each
				 * subsequent dimension always operates on a subset of prior
				 * dimension's hyper slice).
				 */
				if (r.getSliceOperation() == SliceOperation.COMPLEMENT) {
					if (nextSo == SliceOperation.ADD)
						nextSo = SliceOperation.COMPLEMENT;
					else
						nextSo = SliceOperation.ADD;
				}
				if (partialSpec.size() == dimIndex)
					partialSpec.add(r);
				else
					partialSpec.set(dimIndex, r);
				generateScanSpecs(cuboid, scanHolder, partialSpec, dimIndex + 1, groupKeyLen, nextSo, measureQualifiers);
			}

		}
	}

	protected Cuboid findCuboid() {

		/*
		 * we need to find cuboid with composite keys where grouping dimensions
		 * are stacked on the left but where all of the sliced dimensions are
		 * also present (in any position).
		 */

		Set<String> dimensionSubset = new HashSet<String>();
		dimensionSubset.addAll(dimSlices.keySet());
		dimensionSubset.addAll(groupDimensions);

		Cuboid cuboid = null;

		// find a suitable cuboid per above with fewest extra dimensions.
		for (Cuboid c : cube.getCuboids()) {

			List<String> cPath = c.getCuboidPath();
			// filter out smaller cubes
			if (cPath.size() < dimensionSubset.size())
				continue;

			// filter out those that don't contain all the dimensions we need
			if (!cPath.containsAll(dimensionSubset))
				continue;

			// now check group dimensions that must be stacked on the left.
			int cnt = groupDimensions.size();
			for (String dimName : cPath) {
				if (groupDimensions.contains(dimName)) {
					cnt--;
				} else {
					/*
					 * easy but still quite effective optimization is that if
					 * slices are degenerate, their dimension could be pushed
					 * left in the cuboid without breaking inline grouping
					 * prerequisites. This is surprisingly much more often the
					 * case as degenerate slicing is quite common.
					 */
					List<Slice> slices = dimSlices.get(dimName);
					if (slices == null || slices.size() != 1)
						break; // clearly not degenerate

					Slice slice = slices.get(0);
					if (slice.isLeftOpen() || slice.isRightOpen()
							|| !slice.getLeftBound().equals(slice.getRightBound()))
						break; // not degenerate.
				}
			}
			if (cnt > 0)
				continue;

			// found qualifying cuboid. good.
			if (cuboid == null || cPath.size() < cuboid.getCuboidPath().size()) {
				if(distinctDimension != null) {
				    boolean foundDC = false;
				    boolean success = true;
				    for(Dimension d : c.getCuboidDimensions()) {
				        if(foundDC) {
				            success = success ? !dimensionSubset.contains(d.getName()) : false;
				        } else if(d.getName().equals(distinctDimension)) {
				            foundDC = true;
				        }
				    }
				    if(success) {
				        cuboid = c;
				    }
				} else {
				    cuboid = c;
				}
			}
		}

		return cuboid;

	}

	@Override
	public AggregateQueryImpl addOrderBy(String dimension, boolean ascending) {

		int numGroupKeys = groupDimensions.size();
		int groupKeyLen = 0;

		for (int i = 0; i < numGroupKeys; i++) {

			Dimension dim = findCuboid().getCuboidDimensions().get(i);
			if(dim.getName().equals(dimension)) {
				RawScanResultMeasureComparator obm = new RawScanResultMeasureComparator(groupKeyLen, groupKeyLen+dim.getKeyLen(), ascending);
				comparator.add(obm);
			}
			groupKeyLen += dim.getKeyLen();
		}

		return this;
	}

	@Override
	public AggregateQuery addOrderByMeasure(String function, String measureName, boolean ascending) {
		if(function.equalsIgnoreCase("DCOUNT")) {
			RawScanResultMeasureComparator obm = new RawScanResultMeasureComparator(ascending);
			comparator.add(obm);
		} else {
			measures.add(measureName);

			int mCnt=0;
			for (String mName : measures) {
				if(measureName.equals(mName)) {
					RawScanResultMeasureComparator obm = new RawScanResultMeasureComparator(mCnt, function, ascending);
					comparator.add(obm);
				}
				mCnt++;
			}
		}
		return this;
	}

	@Override
	public AggregateQuery addLimit(int _limit) {
		limit = _limit;
		return this;
	}

	@Override
	public AggregateQuery addDistinctCount(String _dimension, String alias) {
		distinctDimension = _dimension;
		groupDimensions.add(_dimension);
		
		return this;
	}

}
